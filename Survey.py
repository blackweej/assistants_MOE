"""
Survey System for Dual MoE Pipeline
ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ê³  fusion_degreeë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ì‹œìŠ¤í…œ

ì‘ì„±ì: Assistant
ë‚ ì§œ: 2025ë…„ 7ì›”
"""

import json
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np
from pathlib import Path
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SurveyResponse:
    """ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    question: str
    first_output_text: str
    second_output_text: str
    fused_output_text: str
    relevance_scores: List[int]  # 5ê°œì˜ ì ìˆ˜ (1-5)
    timestamp: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()

@dataclass
class FeedbackMetrics:
    """í”¼ë“œë°± ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    expert_id: int
    avg_relevance: float
    feedback_count: int
    trend: str  # 'positive', 'negative', 'neutral'
    last_updated: str
    confidence_score: float
    
    def __post_init__(self):
        if self.last_updated is None:
            self.last_updated = datetime.now().isoformat()

class SurveySystem:
    """
    ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° fusion_degree ë™ì  ì¡°ì • ì‹œìŠ¤í…œ
    """
    
    def __init__(self, 
                 fusion_controller=None,
                 survey_storage_path: str = "survey_data.json",
                 adjustment_rates: Dict[str, float] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            fusion_controller: FusionController ì¸ìŠ¤í„´ìŠ¤
            survey_storage_path: ì„¤ë¬¸ ë°ì´í„° ì €ì¥ ê²½ë¡œ
            adjustment_rates: ì¡°ì •ìœ¨ ì„¤ì •
        """
        self.fusion_controller = fusion_controller
        self.survey_storage_path = Path(survey_storage_path)
        
        # ê¸°ë³¸ ì¡°ì •ìœ¨ ì„¤ì •
        self.adjustment_rates = adjustment_rates or {
            'positive': 0.05,  # ê¸ì •ì  í”¼ë“œë°± ì‹œ ì¦ê°€ìœ¨
            'negative': -0.08, # ë¶€ì •ì  í”¼ë“œë°± ì‹œ ê°ì†Œìœ¨
            'neutral': 0.0
        }
        
        # ì„¤ë¬¸ ì‘ë‹µ ì €ì¥ì†Œ
        self.survey_responses: List[SurveyResponse] = []
        
        # í”¼ë“œë°± ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.feedback_metrics: Dict[int, FeedbackMetrics] = {}
        
        # ë°ì´í„° ë¡œë”©
        self._load_survey_data()
        
        logger.info(f"Survey System initialized with storage path: {self.survey_storage_path}")

    def collect_survey_response(self, 
                              question: str,
                              outputs: Dict[str, str],
                              user_id: Optional[str] = None,
                              session_id: Optional[str] = None,
                              auto_mode: bool = False) -> SurveyResponse:
        """
        ì‚¬ìš©ìë¡œë¶€í„° ì„¤ë¬¸ ì‘ë‹µ ìˆ˜ì§‘
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            outputs: {'first': str, 'second': str, 'fused': str}
            user_id: ì‚¬ìš©ì ID (ì„ íƒì )
            session_id: ì„¸ì…˜ ID (ì„ íƒì )
            auto_mode: ìë™ ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©)
        
        Returns:
            SurveyResponse ì¸ìŠ¤í„´ìŠ¤
        """
        logger.info(f"Collecting survey response for question: {question[:50]}...")
        
        # ì„¤ë¬¸ ì‘ë‹µ ê°ì²´ ìƒì„±
        survey_response = SurveyResponse(
            question=question,
            first_output_text=outputs.get('first', ''),
            second_output_text=outputs.get('second', ''),
            fused_output_text=outputs.get('fused', ''),
            relevance_scores=[],
            timestamp=datetime.now().isoformat(),
            user_id=user_id,
            session_id=session_id
        )
        
        if auto_mode:
            # ìë™ ëª¨ë“œ: ëœë¤ ì ìˆ˜ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
            survey_response.relevance_scores = self._generate_auto_scores(outputs)
        else:
            # ì‹¤ì œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
            relevance_scores = self._display_survey_interface(survey_response)
            survey_response.relevance_scores = relevance_scores
        
        # ì‘ë‹µ ì €ì¥
        self.survey_responses.append(survey_response)
        
        # ì¦‰ì‹œ fusion_degree ì—…ë°ì´íŠ¸
        self._update_fusion_degrees_from_response(survey_response)
        
        # ë°ì´í„° ì €ì¥
        self._save_survey_data()
        
        logger.info(f"Survey response collected with scores: {survey_response.relevance_scores}")
        return survey_response

    def _display_survey_interface(self, survey_response: SurveyResponse) -> List[int]:
        """
        ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ì„¤ë¬¸ ì§„í–‰
        
        Args:
            survey_response: SurveyResponse ì¸ìŠ¤í„´ìŠ¤
        
        Returns:
            List[int]: 5ê°œì˜ ì—°ê´€ì„± ì ìˆ˜ (1-5)
        """
        print("\n" + "="*80)
        print("ğŸ“‹ SURVEY: ì‘ë‹µ í’ˆì§ˆ í‰ê°€")
        print("="*80)
        
        print(f"\nğŸ’­ ì›ë³¸ ì§ˆë¬¸: {survey_response.question}")
        print("\n" + "-"*50)
        
        # 3ê°€ì§€ ì¶œë ¥ í‘œì‹œ
        outputs = [
            ("ì²« ë²ˆì§¸ ì¶œë ¥ (Original MoE)", survey_response.first_output_text),
            ("ë‘ ë²ˆì§¸ ì¶œë ¥ (Assistant Enhanced)", survey_response.second_output_text),
            ("ìœµí•© ì¶œë ¥ (Fused Output)", survey_response.fused_output_text)
        ]
        
        for i, (title, text) in enumerate(outputs, 1):
            print(f"\n{i}. {title}:")
            print(f"   {text[:200]}..." if len(text) > 200 else f"   {text}")
        
        print("\n" + "-"*50)
        print("ğŸ“Š ë‹¤ìŒ ê° í•­ëª©ì— ëŒ€í•´ 1-5ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:")
        print("   (1=ë§¤ìš° ë‚®ìŒ, 2=ë‚®ìŒ, 3=ë³´í†µ, 4=ë†’ìŒ, 5=ë§¤ìš° ë†’ìŒ)")
        
        questions = [
            "ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„±",
            "ì‘ë‹µì˜ ìœ ìš©ì„±",
            "ì •ë³´ì˜ ì •í™•ì„±",
            "ì°½ì˜ì„±/ì°¸ì‹ ì„±",
            "ì „ì²´ì  ë§Œì¡±ë„"
        ]
        
        scores = []
        for i, question in enumerate(questions, 1):
            while True:
                try:
                    score = int(input(f"{i}. {question}: "))
                    if 1 <= score <= 5:
                        scores.append(score)
                        break
                    else:
                        print("   âŒ 1-5 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                except ValueError:
                    print("   âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ì ìˆ˜: {scores}")
        print("="*80)
        
        return scores

    def _generate_auto_scores(self, outputs: Dict[str, str]) -> List[int]:
        """
        ìë™ ì ìˆ˜ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            outputs: ì¶œë ¥ í…ìŠ¤íŠ¸ë“¤
        
        Returns:
            List[int]: 5ê°œì˜ ì—°ê´€ì„± ì ìˆ˜
        """
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê°„ë‹¨í•œ ì ìˆ˜ ìƒì„±
        fused_len = len(outputs.get('fused', ''))
        first_len = len(outputs.get('first', ''))
        second_len = len(outputs.get('second', ''))
        
        # ê¸°ë³¸ ì ìˆ˜ (3) + ê¸¸ì´ ê¸°ë°˜ ë³´ì •
        base_score = 3
        
        # ìœµí•© ì¶œë ¥ì´ ë” ê¸¸ë©´ ì¢‹ì€ ì ìˆ˜
        if fused_len > max(first_len, second_len):
            bonus = 1
        elif fused_len < min(first_len, second_len):
            bonus = -1
        else:
            bonus = 0
        
        # ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€
        import random
        scores = []
        for _ in range(5):
            score = base_score + bonus + random.randint(-1, 1)
            scores.append(max(1, min(5, score)))
        
        return scores

    def _update_fusion_degrees_from_response(self, survey_response: SurveyResponse):
        """
        ë‹¨ì¼ ì„¤ë¬¸ ì‘ë‹µìœ¼ë¡œë¶€í„° fusion_degree ì—…ë°ì´íŠ¸
        
        Args:
            survey_response: SurveyResponse ì¸ìŠ¤í„´ìŠ¤
        """
        if not self.fusion_controller:
            logger.warning("FusionController not available, skipping fusion degree update")
            return
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_relevance = sum(survey_response.relevance_scores) / len(survey_response.relevance_scores)
        
        # ëª¨ë“  active expertì— ëŒ€í•´ ì—…ë°ì´íŠ¸
        for expert_id in range(8):  # ê¸°ë³¸ 8ê°œ expert ê°€ì •
            # í˜„ì¬ fusion_degree ê°€ì ¸ì˜¤ê¸°
            current_degree = getattr(self.fusion_controller, 'fusion_degrees', {}).get(expert_id, 1.0)
            
            # ì¡°ì •ìœ¨ ê³„ì‚°
            adjustment_rate = self._calculate_adjustment_rate(avg_relevance, 'neutral')
            
            # ìƒˆë¡œìš´ fusion_degree ê³„ì‚°
            new_degree = self._apply_adjustment(current_degree, adjustment_rate)
            
            # ì—…ë°ì´íŠ¸ (FusionControllerê°€ ìˆì„ ê²½ìš°)
            if hasattr(self.fusion_controller, 'fusion_degrees'):
                self.fusion_controller.fusion_degrees[expert_id] = new_degree
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_feedback_metrics(expert_id, avg_relevance)
        
        logger.info(f"Fusion degrees updated based on survey response (avg score: {avg_relevance:.2f})")

    def update_fusion_degrees_batch(self, survey_responses: List[SurveyResponse]) -> List[FeedbackMetrics]:
        """
        ë°°ì¹˜ ì„¤ë¬¸ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ fusion_degree ì—…ë°ì´íŠ¸
        
        Args:
            survey_responses: ì„¤ë¬¸ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[FeedbackMetrics]: ì—…ë°ì´íŠ¸ëœ ë©”íŠ¸ë¦­ë“¤
        """
        logger.info(f"Updating fusion degrees from {len(survey_responses)} survey responses")
        
        # Expertë³„ í”¼ë“œë°± ì§‘ê³„
        expert_feedback = self._aggregate_feedback_by_expert(survey_responses)
        
        updated_metrics = []
        for expert_id, feedback_data in expert_feedback.items():
            # í‰ê·  ì—°ê´€ì„± ì ìˆ˜ ê³„ì‚°
            avg_relevance = sum(feedback_data['scores']) / len(feedback_data['scores'])
            
            # íŠ¸ë Œë“œ ë¶„ì„
            trend = self._analyze_trend(feedback_data['scores'])
            
            # Fusion degree ì¡°ì •
            adjustment_rate = self._calculate_adjustment_rate(avg_relevance, trend)
            
            current_degree = getattr(self.fusion_controller, 'fusion_degrees', {}).get(expert_id, 1.0)
            new_degree = self._apply_adjustment(current_degree, adjustment_rate)
            
            # ì—…ë°ì´íŠ¸
            if hasattr(self.fusion_controller, 'fusion_degrees'):
                self.fusion_controller.fusion_degrees[expert_id] = new_degree
            
            # ë©”íŠ¸ë¦­ ìƒì„±
            metrics = FeedbackMetrics(
                expert_id=expert_id,
                avg_relevance=avg_relevance,
                feedback_count=len(feedback_data['scores']),
                trend=trend,
                last_updated=datetime.now().isoformat(),
                confidence_score=self._calculate_confidence_score(feedback_data['scores'])
            )
            
            updated_metrics.append(metrics)
            self.feedback_metrics[expert_id] = metrics
        
        # ë°ì´í„° ì €ì¥
        self._save_survey_data()
        
        logger.info(f"Batch fusion degree update completed for {len(updated_metrics)} experts")
        return updated_metrics

    def _aggregate_feedback_by_expert(self, survey_responses: List[SurveyResponse]) -> Dict[int, Dict]:
        """
        Expertë³„ í”¼ë“œë°± ì§‘ê³„
        
        Args:
            survey_responses: ì„¤ë¬¸ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Dict[int, Dict]: Expert IDë³„ ì§‘ê³„ ë°ì´í„°
        """
        expert_feedback = {}
        
        for response in survey_responses:
            avg_score = sum(response.relevance_scores) / len(response.relevance_scores)
            
            # ëª¨ë“  expertì— ëŒ€í•´ ì§‘ê³„ (ì‹¤ì œë¡œëŠ” í•´ë‹¹ ì‘ë‹µê³¼ ê´€ë ¨ëœ expertë§Œ)
            for expert_id in range(8):  # ê¸°ë³¸ 8ê°œ expert ê°€ì •
                if expert_id not in expert_feedback:
                    expert_feedback[expert_id] = {'scores': [], 'timestamps': []}
                
                expert_feedback[expert_id]['scores'].append(avg_score)
                expert_feedback[expert_id]['timestamps'].append(response.timestamp)
        
        return expert_feedback

    def _analyze_trend(self, scores: List[float]) -> str:
        """
        ì ìˆ˜ íŠ¸ë Œë“œ ë¶„ì„
        
        Args:
            scores: ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            str: 'positive', 'negative', 'neutral'
        """
        if len(scores) < 2:
            return 'neutral'
        
        # ìµœê·¼ 5ê°œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ë Œë“œ ë¶„ì„
        recent_scores = scores[-5:]
        
        if len(recent_scores) < 2:
            return 'neutral'
        
        # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(recent_scores))
        y = np.array(recent_scores)
        
        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            
            if slope > 0.1:
                return 'positive'
            elif slope < -0.1:
                return 'negative'
            else:
                return 'neutral'
        
        return 'neutral'

    def _calculate_adjustment_rate(self, avg_relevance: float, trend: str) -> float:
        """
        í‰ê·  ì—°ê´€ì„±ê³¼ íŠ¸ë Œë“œ ê¸°ë°˜ ì¡°ì •ìœ¨ ê³„ì‚°
        
        Args:
            avg_relevance: í‰ê·  ì—°ê´€ì„± ì ìˆ˜ (1-5)
            trend: íŠ¸ë Œë“œ ('positive', 'negative', 'neutral')
        
        Returns:
            float: ì¡°ì •ìœ¨
        """
        # ê¸°ë³¸ ì¡°ì •ìœ¨ (5ì  ë§Œì  ê¸°ì¤€)
        base_rate = (avg_relevance - 3.0) / 2.0  # -1 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        # íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
        trend_multiplier = {
            'positive': 1.2,
            'negative': 1.5,  # ë¶€ì •ì  í”¼ë“œë°±ì— ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘
            'neutral': 1.0
        }
        
        adjustment_rate = base_rate * trend_multiplier[trend]
        
        # ì¡°ì •ìœ¨ ì œí•œ
        if adjustment_rate > 0:
            adjustment_rate = min(adjustment_rate, self.adjustment_rates['positive'])
        else:
            adjustment_rate = max(adjustment_rate, self.adjustment_rates['negative'])
        
        return adjustment_rate

    def _apply_adjustment(self, current_degree: float, adjustment_rate: float) -> float:
        """
        í˜„ì¬ fusion_degreeì— ì¡°ì •ìœ¨ ì ìš©
        
        Args:
            current_degree: í˜„ì¬ fusion_degree (0-1)
            adjustment_rate: ì¡°ì •ìœ¨
        
        Returns:
            float: ìƒˆë¡œìš´ fusion_degree (0-1)
        """
        new_degree = current_degree + adjustment_rate
        
        # ë²”ìœ„ ì œí•œ
        new_degree = max(0.0, min(1.0, new_degree))
        
        return new_degree

    def _calculate_confidence_score(self, scores: List[float]) -> float:
        """
        í”¼ë“œë°± ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        
        Args:
            scores: ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            float: ì‹ ë¢°ë„ ì ìˆ˜ (0-1)
        """
        if len(scores) < 2:
            return 0.5
        
        # ë¶„ì‚° ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚° (ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ)
        variance = np.var(scores)
        max_variance = 4.0  # 1-5 ì ìˆ˜ì—ì„œ ìµœëŒ€ ë¶„ì‚°
        
        confidence = 1.0 - (variance / max_variance)
        return max(0.0, min(1.0, confidence))

    def _update_feedback_metrics(self, expert_id: int, avg_relevance: float):
        """
        ê°œë³„ Expertì˜ í”¼ë“œë°± ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        
        Args:
            expert_id: Expert ID
            avg_relevance: í‰ê·  ì—°ê´€ì„± ì ìˆ˜
        """
        if expert_id not in self.feedback_metrics:
            self.feedback_metrics[expert_id] = FeedbackMetrics(
                expert_id=expert_id,
                avg_relevance=avg_relevance,
                feedback_count=1,
                trend='neutral',
                last_updated=datetime.now().isoformat(),
                confidence_score=0.5
            )
        else:
            metrics = self.feedback_metrics[expert_id]
            # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
            metrics.avg_relevance = (metrics.avg_relevance * metrics.feedback_count + avg_relevance) / (metrics.feedback_count + 1)
            metrics.feedback_count += 1
            metrics.last_updated = datetime.now().isoformat()

    def _save_survey_data(self):
        """ì„¤ë¬¸ ë°ì´í„° ì €ì¥"""
        try:
            data = {
                'survey_responses': [asdict(response) for response in self.survey_responses],
                'feedback_metrics': {str(k): asdict(v) for k, v in self.feedback_metrics.items()},
                'last_updated': datetime.now().isoformat()
            }
            
            with open(self.survey_storage_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Survey data saved to {self.survey_storage_path}")
        except Exception as e:
            logger.error(f"Failed to save survey data: {e}")

    def _load_survey_data(self):
        """ì„¤ë¬¸ ë°ì´í„° ë¡œë”©"""
        try:
            if self.survey_storage_path.exists():
                with open(self.survey_storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ì„¤ë¬¸ ì‘ë‹µ ë¡œë”©
                self.survey_responses = [
                    SurveyResponse(**response) 
                    for response in data.get('survey_responses', [])
                ]
                
                # í”¼ë“œë°± ë©”íŠ¸ë¦­ ë¡œë”©
                metrics_data = data.get('feedback_metrics', {})
                self.feedback_metrics = {
                    int(k): FeedbackMetrics(**v) 
                    for k, v in metrics_data.items()
                }
                
                logger.info(f"Survey data loaded: {len(self.survey_responses)} responses, {len(self.feedback_metrics)} metrics")
            else:
                logger.info("No existing survey data found, starting fresh")
        except Exception as e:
            logger.error(f"Failed to load survey data: {e}")

    def get_survey_statistics(self) -> Dict:
        """ì„¤ë¬¸ í†µê³„ ë°˜í™˜"""
        if not self.survey_responses:
            return {'total_responses': 0, 'average_score': 0.0, 'expert_metrics': {}}
        
        total_responses = len(self.survey_responses)
        all_scores = []
        
        for response in self.survey_responses:
            all_scores.extend(response.relevance_scores)
        
        average_score = sum(all_scores) / len(all_scores) if all_scores else 0.0
        
        return {
            'total_responses': total_responses,
            'average_score': average_score,
            'expert_metrics': {k: asdict(v) for k, v in self.feedback_metrics.items()},
            'recent_trends': self._get_recent_trends()
        }

    def _get_recent_trends(self) -> Dict:
        """ìµœê·¼ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.survey_responses) < 5:
            return {'trend': 'insufficient_data', 'confidence': 0.0}
        
        recent_responses = self.survey_responses[-10:]  # ìµœê·¼ 10ê°œ ì‘ë‹µ
        recent_scores = []
        
        for response in recent_responses:
            avg_score = sum(response.relevance_scores) / len(response.relevance_scores)
            recent_scores.append(avg_score)
        
        trend = self._analyze_trend(recent_scores)
        confidence = self._calculate_confidence_score(recent_scores)
        
        return {
            'trend': trend,
            'confidence': confidence,
            'recent_average': sum(recent_scores) / len(recent_scores),
            'score_variance': np.var(recent_scores)
        }

    def export_survey_data(self, export_path: str = None) -> str:
        """ì„¤ë¬¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        if export_path is None:
            export_path = f"survey_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        statistics = self.get_survey_statistics()
        
        export_data = {
            'metadata': {
                'export_time': datetime.now().isoformat(),
                'total_responses': len(self.survey_responses),
                'system_version': '1.0.0'
            },
            'statistics': statistics,
            'raw_data': {
                'survey_responses': [asdict(response) for response in self.survey_responses],
                'feedback_metrics': {str(k): asdict(v) for k, v in self.feedback_metrics.items()}
            }
        }
        
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Survey data exported to {export_path}")
        return export_path

# ì‚¬ìš© ì˜ˆì œ ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ FusionController ëª¨í‚¹
    class MockFusionController:
        def __init__(self):
            self.fusion_degrees = {i: 1.0 for i in range(8)}
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    fusion_controller = MockFusionController()
    survey_system = SurveySystem(fusion_controller)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_outputs = {
        'first': "ì´ê²ƒì€ ì²« ë²ˆì§¸ MoE ì¶œë ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
        'second': "ì´ê²ƒì€ Assistantê°€ ê°•í™”í•œ ë‘ ë²ˆì§¸ ì¶œë ¥ì…ë‹ˆë‹¤. ë” ì°½ì˜ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
        'fused': "ì´ê²ƒì€ ìœµí•©ëœ ìµœì¢… ì¶œë ¥ì…ë‹ˆë‹¤. ë‘ ì¶œë ¥ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
    }
    
    print("ğŸ”§ Survey System í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ìë™ ëª¨ë“œë¡œ ì„¤ë¬¸ ì‘ë‹µ ìˆ˜ì§‘
    response = survey_system.collect_survey_response(
        question="ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        outputs=test_outputs,
        user_id="test_user",
        auto_mode=True
    )
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ëœ ì‘ë‹µ: {response.relevance_scores}")
    
    # í†µê³„ í™•ì¸
    stats = survey_system.get_survey_statistics()
    print(f"\nğŸ“ˆ í˜„ì¬ í†µê³„: {stats}")
    
    # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    export_path = survey_system.export_survey_data()
    print(f"\nğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_path}")
    
    print("\nâœ… Survey System í…ŒìŠ¤íŠ¸ ì™„ë£Œ")